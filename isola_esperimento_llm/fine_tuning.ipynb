{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torch e CUDA\n",
    "import torch\n",
    "import gc\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "# Transformers e Training\n",
    "from transformers import (\n",
    "    TextStreamer,\n",
    "    TrainingArguments,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "from trl import SFTTrainer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "# Dataset e valutazione\n",
    "from datasets import load_dataset, Dataset\n",
    "from evaluate import load\n",
    "import bitsandbytes as bnb\n",
    "# Metriche di valutazione\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_recall_fscore_support,\n",
    "    confusion_matrix\n",
    ")\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Sistema e utility\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Visualizzazione\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name=\"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "if \"meta\" in model_name:\n",
    "    prompt_template = (\n",
    "        \"### Instruction:\\n\"\n",
    "        \"You are an expert software developer and bug triaging specialist. Your task is to predict whether a bug \"\n",
    "        \"will be resolved in LESS than 50 DAYS or MORE than 50 DAYS based on the provided bug details.\\n\\n\"\n",
    "        \n",
    "        \"- Output '0' if the bug will be resolved in LESS than 50 DAYS.\\n\"\n",
    "        \"- Output '1' if the bug will be resolved in MORE than 50 DAYS.\\n\\n\"\n",
    "        \n",
    "        \"Your response MUST be strictly either '0' or '1'. Do NOT include any additional text, explanations, formatting, symbols, or extra characters in your response.\\n\\n\"\n",
    "\n",
    "        \"### Input:\\n\"\n",
    "        \"Source: {source}\\n\"\n",
    "        \"Product: {product}\"\n",
    "        \"Short Description: {short_desc}\\n\"\n",
    "        \"Priority: {priority}\\n\"\n",
    "        \"Severity: {bug_severity}\\n\"\n",
    "        #\"Estimated resolution time: {days_resolution}\\n\\n\" - questo potrebbe influenzare troppo il modello per la predizione\n",
    "\n",
    "        \"### Example Responses:\\n\"\n",
    "        \"Input: Source: KDE | Product: Payment System | Short Description: Critical security vulnerability found in authentication system | Priority: P1 | Severity: Critical\\n\"\n",
    "        \"Output: 0\\n\\n\"\n",
    "        \"Input: Source: OpenOffice | Product: UI Module | Short Description: UI glitch affecting low-impact visual elements in settings panel | Priority: P3 | Severity: Minor\\n\"\n",
    "        \"Output: 1\\n\\n\"\n",
    "\n",
    "        \"### Output: {label}\\n\"\n",
    "    )\n",
    "else:\n",
    "    prompt_template = (\n",
    "        \"You are an expert software developer and bug triaging specialist. Your task is to predict whether a bug \"\n",
    "        \"will be resolved in LESS than 50 DAYS or MORE than 50 DAYS based on the provided bug details.\\n\\n\"\n",
    "        \n",
    "        \"- Output '0' if the bug will be resolved in LESS than 50 DAYS.\\n\"\n",
    "        \"- Output '1' if the bug will be resolved in MORE than 50 DAYS.\\n\\n\"\n",
    "        \n",
    "        \"Your response MUST be strictly either '0' or '1'. Do NOT include any additional text, explanations, formatting, symbols, or extra characters in your response.\\n\\n\"\n",
    "\n",
    "        \"## Input:\\n\"\n",
    "        \"Source: {source}\\n\"\n",
    "        \"Product: {product}\"\n",
    "        \"Short Description: {short_desc}\\n\"\n",
    "        \"Priority: {priority}\\n\"\n",
    "        \"Severity: {bug_severity}\\n\"\n",
    "        #\"Estimated resolution time: {days_resolution}\\n\\n\" - questo potrebbe influenzare troppo il modello per la predizione\n",
    "\n",
    "        \"## Example Responses:\\n\"\n",
    "        \"Input: Source: KDE | Product: Payment System | Short Description: Critical security vulnerability found in authentication system | Priority: P1 | Severity: Critical\\n\"\n",
    "        \"0\\n\\n\"\n",
    "        \"Input: Source: OpenOffice | Product: UI Module | Short Description: UI glitch affecting low-impact visual elements in settings panel | Priority: P3 | Severity: Minor\\n\"\n",
    "        \"1\\n\\n\"\n",
    "\n",
    "        \"## Output:\"\n",
    "        \"{label}\"\n",
    "    )\n",
    "num_val = \"2000\" #1000, 2000, 5000, 9000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "caricamento del modello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 2048\n",
    "dtype = torch.float16 #altrimenti None\n",
    "load_in_4bit = True\n",
    "seed = 3407\n",
    "load_dotenv()\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "#model_name=\"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "#model_name=\"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token = hf_token)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Peft -  Parameter Efficient Fine Tuning\n",
    "# LoRA config\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    # Layer fondamentali per catturare relazioni tra token\n",
    "    # q_proj : \"Query projection\", v_proj: \"Value projection\", k_proj : \"Key projection\", o_proh: \"output projection\"\n",
    "    #target_modules=['q_proj', 'v_proj', 'k_proj','o_proj','gate_proj','up_proj','down_proj','lm_head','embedded_layers']\n",
    "    target_modules = ['q_proj', 'v_proj', 'gate_proj', 'up_proj', 'down_proj'] #forse lm_head non serve perchÃ¨ generiamo solo un singolo token\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()\n",
    "model.gradient_checkpointing_enable()\n",
    "model.enable_input_require_grads()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "formattazione del prompt con i dati del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EOS_TOKEN = tokenizer.eos_token  # Assicuriamoci di aggiungere il token EOS alla fine\n",
    "\n",
    "def formatting_prompts(examples, include_label=True):\n",
    "    texts = []\n",
    "    for source, product, short_desc, priority, bug_severity, label in zip(\n",
    "        examples[\"source\"],examples[\"product\"], examples[\"short_desc\"], examples[\"priority\"], examples[\"bug_severity\"], examples[\"label\"]\n",
    "    ):\n",
    "        if include_label:\n",
    "            text = prompt_template.format(\n",
    "                source=source,\n",
    "                product=product,\n",
    "                short_desc=short_desc,\n",
    "                priority=priority,\n",
    "                bug_severity=bug_severity,\n",
    "                label=label,  # La label viene passata solo se include_label=True\n",
    "            ) + EOS_TOKEN\n",
    "        else:\n",
    "            text = prompt_template.format(\n",
    "                source=source,\n",
    "                product=product, \n",
    "                short_desc=short_desc,\n",
    "                priority=priority,\n",
    "                bug_severity=bug_severity,\n",
    "                label=\"\",  #  Non passiamo la label\n",
    "            ) + EOS_TOKEN\n",
    "        \n",
    "        texts.append(text)\n",
    "    \n",
    "    return {\"text\": texts}\n",
    "\n",
    "\n",
    "# Caricamento dataset\n",
    "dataset = load_dataset(\n",
    "    \"csv\",\n",
    "    data_files={\n",
    "        \"train\": f\"../dataset_completo/balanced_datasets/balanced_train_{num_val}.csv\", \n",
    "        \"test\": f\"../dataset_completo/balanced_datasets/balanced_test.csv\", \n",
    "        \"val\": f\"../dataset_completo/balanced_datasets/balanced_validation.csv\" \n",
    "    },\n",
    ")\n",
    "\n",
    "# Formattiamo il dataset con il nuovo prompt\n",
    "# Applichiamo la funzione al dataset\n",
    "dataset[\"train\"] = dataset[\"train\"].map(lambda x: formatting_prompts(x, include_label=True), batched=True)\n",
    "dataset[\"val\"] = dataset[\"val\"].map(lambda x: formatting_prompts(x, include_label=False), batched=True)  # ðŸš¨ Label nascosta\n",
    "#dataset[\"test\"] = dataset[\"test\"].map(lambda x: formatting_prompts(x, include_label=False), batched=True)  # ðŸš¨ Label nascosta\n",
    "\n",
    "\n",
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-tuning del modello"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LoRA (Low-Rank Adaptation) Ã¨ una tecnica di adattamento di modelli di linguaggio, come GPT o altri LLM (Large Language Models), che permette di fare il fine-tuning del modello con un numero ridotto di parametri, mantenendo la prestazione e riducendo il costo computazionale. La sua filosofia si basa sull'idea che solo una piccola parte della rete neurale necessiti di adattamenti per specializzarsi su un compito specifico, anzichÃ© dover modificare l'intero modello.\n",
    "Concetto di base\n",
    "\n",
    "In LoRA, il modello pre-addestrato non viene modificato nei suoi pesi originali, ma si aggiungono matrici di adattamento a basso rango. Queste matrici vengono apprese durante il fine-tuning, consentendo di adattare il comportamento del modello a nuovi dati senza dover riaddestrare completamente l'intero sistema.\n",
    "Matematicamente:\n",
    "\n",
    "Consideriamo una rete neurale come una sequenza di trasformazioni lineari, per esempio una mappatura tra uno strato X e Y tramite una matrice di pesi W:\n",
    "\n",
    "Y = W X\n",
    "\n",
    "In un contesto tradizionale di fine-tuning, andremmo a ottimizzare W, ovvero i pesi del modello. Invece, LoRA modifica l'approccio. Per adattare un modello con LoRA, l'idea Ã¨ di decomporre il cambiamento nei pesi in un prodotto di due matrici di basso rango.\n",
    "\n",
    "    Decomposizione del cambiamento nei pesi: Invece di aggiornare direttamente la matrice di pesi W, LoRA introduce due nuove matrici A e B, dove A ha dimensioni d Ã— r (dove d Ã¨ la dimensione dell'input, e r Ã¨ un valore che indica il rango ridotto) e B ha dimensioni r Ã— d (dove d Ã¨ la dimensione dell'output). L'idea Ã¨ che r Ã¨ molto piÃ¹ piccolo di d, e quindi questo approccio riduce significativamente il numero di parametri da ottimizzare.\n",
    "\n",
    "    Modifica della trasformazione: L'aggiornamento del modello tramite LoRA non avviene direttamente su W, ma sulla sua \"perturbazione\" tramite il prodotto delle matrici A e B:\n",
    "\n",
    "Y = (W + A B) X\n",
    "\n",
    "Dove:\n",
    "\n",
    "    W Ã¨ la matrice dei pesi originale del modello pre-addestrato.\n",
    "    A B Ã¨ l'aggiornamento a basso rango che viene appreso durante il fine-tuning.\n",
    "\n",
    "    Ottimizzazione: Durante il fine-tuning, i pesi A e B vengono aggiornati, mentre W rimane fisso. L'ottimizzazione si concentra esclusivamente su questi due nuovi parametri, che sono molto piÃ¹ piccoli rispetto ai pesi originali.\n",
    "\n",
    "    ProprietÃ  di basso rango: L'idea di utilizzare matrici a basso rango Ã¨ che molte modifiche necessarie per adattare un modello a un nuovo compito possono essere catturate tramite piccole modifiche che sono sufficientemente espresse in uno spazio di dimensioni ridotte. Questo riduce il numero totale di parametri e quindi le risorse computazionali richieste per l'addestramento.\n",
    "\n",
    "Vantaggi:\n",
    "\n",
    "    Efficienza: Non Ã¨ necessario riaddestrare l'intero modello, solo le matrici A e B, che sono di dimensioni molto piÃ¹ piccole.\n",
    "    Memoria: PoichÃ© A e B sono di basso rango, richiedono meno memoria e meno spazio per essere memorizzate e aggiornate.\n",
    "    VersatilitÃ : PoichÃ© W rimane fisso, il modello pre-addestrato puÃ² essere riutilizzato per piÃ¹ compiti con differenti adattamenti.\n",
    "\n",
    "Riepilogo matematico:\n",
    "\n",
    "    L'aggiornamento della rete avviene come Y = (W + A B) X, dove A e B sono appresi durante il fine-tuning, e il rango di A e B Ã¨ molto piÃ¹ basso rispetto a quello di W.\n",
    "    Il numero di parametri che devono essere ottimizzati Ã¨ molto inferiore rispetto al caso tradizionale di fine-tuning dell'intero modello, il che consente di risparmiare risorse computazionali.\n",
    "\n",
    "LoRA quindi permette di adattare i modelli in modo piÃ¹ efficiente e con meno parametri rispetto ai metodi tradizionali di fine-tuning, mantenendo alta la qualitÃ  e la prestazione nel nuovo compito."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "# Carichiamo la metrica di accuracy\n",
    "#accuracy_metric = load(\"accuracy\")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token  # âœ… Fix padding issue\n",
    "    tokenizer.padding_side = \"right\"\n",
    "model.train() \n",
    "directory = f\"{model_name}\".split(\"/\")[-1].strip()\n",
    "# ðŸ”¹ Configurazione per l'addestramento (usando SFTConfig)\n",
    "sft_config = SFTConfig(\n",
    "    output_dir=f\"{directory}_{num_val}_ft\",\n",
    "    max_seq_length=2048,\n",
    "    dataset_text_field=\"text\",  # Cambia se necessario\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=5,  #  PiÃ¹ epoche per adattare bene LoRA\n",
    "    gradient_accumulation_steps=4,  #  Ridotto per aggiornamenti piÃ¹ frequenti\n",
    "    evaluation_strategy=\"steps\",  #  Valutazione piÃ¹ frequente\n",
    "    eval_steps=100, \n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=1000,\n",
    "    save_total_limit=3,  #  Evita troppi checkpoint\n",
    "    learning_rate=5e-5,  #  Aumentato per migliorare adattamento\n",
    "    lr_scheduler_type=\"cosine\",  #  Cosine decay per convergenza piÃ¹ fluida\n",
    "    warmup_ratio=0.05,  # Warmup ridotto per velocizzare training\n",
    "    fp16=True,  #  Mantieni mixed precision\n",
    "    logging_steps=50,  #  Meno logging per ridurre overhead\n",
    "    metric_for_best_model=\"eval_loss\",  # ðŸ‘ˆ Assicura che il modello salvi in base alla Validation Loss\n",
    "    greater_is_better=False  # ðŸ‘ˆ PerchÃ© una loss minore Ã¨ meglio\n",
    ")\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"val\"],\n",
    "    peft_config=peft_config,\n",
    "    max_seq_length=2048,\n",
    "    dataset_text_field=\"text\",\n",
    "    tokenizer=tokenizer,\n",
    "    args=sft_config,\n",
    "    packing= False,\n",
    ")\n",
    "\n",
    "# Avviamo il training!\n",
    "trainer_stats = trainer.train()\n",
    "eval_results = trainer.evaluate()\n",
    "print(trainer_stats)\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Definiamo le metriche da salvare\n",
    "training_results = {\n",
    "    \"Dataset Size\": num_val,  # Numero di dati usati per il fine-tuning\n",
    "    \"Training Loss\": trainer_stats.training_loss,  # Training Loss\n",
    "    \"Train Time (s)\": trainer_stats.metrics[\"train_runtime\"],  # Tempo di addestramento\n",
    "    \"Steps\": trainer_stats.global_step,  # Numero di passi (steps)\n",
    "    \"Samples/sec\": trainer_stats.metrics[\"train_samples_per_second\"],  # Campioni al secondo\n",
    "    \"Steps/sec\": trainer_stats.metrics[\"train_steps_per_second\"],  # Passi al secondo\n",
    "    \"Validation Loss\": eval_results.get(\"eval_loss\", None),  # Valutazione della loss\n",
    "}\n",
    "\n",
    "# Definiamo il file di destinazione per i risultati\n",
    "results_file = f\"{model_name}_fine_tuned_on_{num_val}/training_comparison.csv\"\n",
    "\n",
    "# Converti il dizionario in un DataFrame\n",
    "training_results_df = pd.DataFrame([training_results])  # Passiamo una lista contenente il dizionario\n",
    "\n",
    "# Assicura che la cartella esista prima di salvare\n",
    "Path(results_file).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Salva il DataFrame nel file CSV, sovrascrivendo se giÃ  esistente\n",
    "training_results_df.to_csv(results_file, index=False)\n",
    "\n",
    "# Mostra la tabella aggiornata\n",
    "print(training_results_df)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = f\"{model_name}\".split(\"/\")[-1].strip().lower()\n",
    "\n",
    "model.save_pretrained(f\"./fine_tuned_model_{directory}_{num_val}\" )\n",
    "tokenizer.save_pretrained(f\"./fine_tuned_model_{directory}_{num_val}\" )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
