{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raffaeleterracino/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Torch e CUDA\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgc\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Subset\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/__init__.py:367\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m USE_GLOBAL_DEPS:\n\u001b[1;32m    366\u001b[0m         _load_global_deps()\n\u001b[0;32m--> 367\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_C\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mSymInt\u001b[39;00m:\n\u001b[1;32m    371\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;124;03m    Like an int (including magic methods), but redirects all operations on the\u001b[39;00m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;124;03m    wrapped node. This is used in particular to symbolically record operations\u001b[39;00m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;124;03m    in the symbolic shape workflow.\u001b[39;00m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:216\u001b[0m, in \u001b[0;36m_lock_unlock_module\u001b[0;34m(name)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Torch e CUDA\n",
    "import torch\n",
    "import gc\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "# Transformers e Training\n",
    "from transformers import (\n",
    "    TextStreamer,\n",
    "    TrainingArguments,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "from trl import SFTTrainer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "# Dataset e valutazione\n",
    "from datasets import load_dataset, Dataset\n",
    "from evaluate import load\n",
    "import bitsandbytes as bnb\n",
    "# Metriche di valutazione\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_recall_fscore_support,\n",
    "    confusion_matrix\n",
    ")\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Sistema e utility\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Visualizzazione\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = (\n",
    "    \"### Instruction:\\n\"\n",
    "    \"You are an expert software developer and bug triaging specialist. Your task is to predict whether a bug \"\n",
    "    \"will be resolved in LESS than 50 DAYS or MORE than 50 DAYS based on the provided bug details.\\n\\n\"\n",
    "    \n",
    "    \"- Output '0' if the bug will be resolved in LESS than 50 DAYS.\\n\"\n",
    "    \"- Output '1' if the bug will be resolved in MORE than 50 DAYS.\\n\\n\"\n",
    "    \n",
    "    \"Your response MUST be strictly either '0' or '1'. Do NOT include any additional text, explanations, formatting, symbols, or extra characters in your response.\\n\\n\"\n",
    "\n",
    "    \"### Input:\\n\"\n",
    "    \"Source: {source}\\n\"\n",
    "    \"Product: {product}\"\n",
    "    \"Short Description: {short_desc}\\n\"\n",
    "    \"Priority: {priority}\\n\"\n",
    "    \"Severity: {bug_severity}\\n\"\n",
    "    #\"Estimated resolution time: {days_resolution}\\n\\n\" - questo potrebbe influenzare troppo il modello per la predizione\n",
    "\n",
    "    \"### Example Responses:\\n\"\n",
    "    \"Input: Source: KDE | Product: Payment System | Short Description: Critical security vulnerability found in authentication system | Priority: P1 | Severity: Critical\\n\"\n",
    "    \"Output: 0\\n\\n\"\n",
    "    \"Input: Source: OpenOffice | Product: UI Module | Short Description: UI glitch affecting low-impact visual elements in settings panel | Priority: P3 | Severity: Minor\\n\"\n",
    "    \"Output: 1\\n\\n\"\n",
    "\n",
    "    \"### Output: {label}\\n\"\n",
    ")\n",
    "num_val = \"1000\" #1000, 2000, 5000, 9000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'short_desc': 'parsetypeDIE confused by DWTAGenumerationtype',\n",
       " 'product': 'valgrind',\n",
       " 'priority': 'NOR',\n",
       " 'bug_severity': 'normal',\n",
       " 'days_resolution': 557,\n",
       " 'comments': 'Version 370 OS Linux The attached small test case makes Valgrind 37 and 36 print parsetypeDIE confused by DWTAGenumerationtype DWATname e DWATdeclaration 1 It seems to be related to ggdb3 Im using gcc 45 so I guess this is not a duplicate of 284124 gcc version gcc UbuntuLinaro 4528ubuntu4 452 Copyright C 2010 Free Software Foundation Inc This is free software see the source for copying conditions There is NO warranty not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE uname a Linux Android 263811generic 50Ubuntu SMP Mon Sep 12 211725 UTC 2011 x8664 x8664 x8664 GNULinux Reproducible Always Steps to Reproduce Run runsh Actual Results WARNING Serious error when reading debug info is printed Expected Results No warning printouts Created attachment 66904 Testcase More testing revealed that its also reproducible with just plain g instead of ggdb3 Does this bug still appear in trunk Yes its reproducible for me on version 12713 on OS X 1068 Still reproducible with Valgrind 381 as included in Ubuntu 1304 both with g and ggdb3 Replicated against valgrind svn trunk and gcc 481 This is a somewhat odd case the DWTAGenumerationtype has a DWATdeclaration attribute meaning it is an incomplete type GCC allows incomplete enum types as GNU extension And it seems they are marked with DWATdeclaration in that case Normally a DWTAGenumerationtype always should have a byte size attribute The given example is slightly strange in that an incomplete enum type is used as the return type of a function prototype Which might only be legal in this special case because the function is never actually usedcalled There is already some code in coregrindmdebuginforeaddwarf3c parsetypeDIE to handle this case but it is only for when the DWTAGenumerationtype is in a separate type unit Do we have something that looks sane if typeETeTyEnumszB 0 we must know the size but not for Ada which uses such dummy enumerations as helper for gdb ada mode parserlanguage A GCC has been seen to put an odd DIE like this into debugtypes DWTAGenumerationtype in debugtypes DWATname indirect string offset 0x3374a execdirectionkind DWATdeclaration 1 It isnt clear what this means but we accept it and assume that the enum is intsized if ccistypeunit typeETeTyEnumszB sizeofint else goto badDIE Maybe instead of the istypeunit check we should just check that it has an declaration attribute and then assume it is sizeofint Or do like we do for structure class and union types and not require a size at all Created attachment 80834 Proposed patch that allows sizeless enum types if they are declarations GCC allows incomplete enums as GNU extension These are marked as DWATdeclaration and wont have a size They can only be used in declaration or as pointer types You cant allocate variables or storage using such an enum type So dont require a size for such enum types Checked in the proposed patch as r13433 Verified as working on revision 13438',\n",
       " 'source': 'KDE',\n",
       " 'label': 1,\n",
       " 'text': \"### Instruction:\\nYou are an expert software developer and bug triaging specialist. Your task is to predict whether a bug will be resolved in LESS than 50 DAYS or MORE than 50 DAYS based on the provided bug details.\\n\\n- Output '0' if the bug will be resolved in LESS than 50 DAYS.\\n- Output '1' if the bug will be resolved in MORE than 50 DAYS.\\n\\nYour response MUST be strictly either '0' or '1'. Do NOT include any additional text, explanations, formatting, symbols, or extra characters in your response.\\n\\n### Input:\\nSource: KDE\\nShort Description: parsetypeDIE confused by DWTAGenumerationtype\\nPriority: NOR\\nSeverity: normal\\n### Example Responses:\\nInput: Source: KDE | Product: Payment System | Short Description: Critical security vulnerability found in authentication system | Priority: P1 | Severity: Critical\\nOutput: 0\\n\\nInput: Source: OpenOffice | Product: UI Module | Short Description: UI glitch affecting low-impact visual elements in settings panel | Priority: P3 | Severity: Minor\\nOutput: 1\\n\\n### Output: \\n\"}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def formatting_prompts(examples):\n",
    "    texts = []\n",
    "    for source,product, short_desc,priority,bug_severity in zip(\n",
    "        examples[\"source\"], examples[\"short_desc\"],examples[\"priority\"],examples[\"bug_severity\"]\n",
    "    ):\n",
    "        # Costruiamo il prompt\n",
    "        text = prompt_template.format(\n",
    "            source=source,\n",
    "            product=product,\n",
    "            short_desc=short_desc,\n",
    "            priority=priority,\n",
    "            bug_severity=bug_severity,\n",
    "            label=\"\",\n",
    "        )\n",
    "\n",
    "        texts.append(text)\n",
    "    \n",
    "    return {\"text\": texts}\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"csv\",\n",
    "    data_files={\n",
    "        \"test\": f\"../dataset_completo/balanced_datasets/balanced_test.csv\", \n",
    "    },\n",
    ")\n",
    "\n",
    "dataset = dataset.map(formatting_prompts, batched=True)\n",
    "dataset['test'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import psutil\n",
    "import pynvml\n",
    "import re\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate_model(model, tokenizer, eval_dataset, model_name, fine_tuned, num_val):\n",
    "    print(\"\\nStarting evaluation phase...\")\n",
    "\n",
    "    # Move model to evaluation mode\n",
    "    model.eval()  \n",
    "\n",
    "    # Ensure tokenizer padding\n",
    "    tokenizer.padding_side = 'left'\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    predictions, true_labels, generated_texts, prediction_sources = [], [], [], []\n",
    "    batch_size = 8\n",
    "    invalid = 0\n",
    "\n",
    "    # Create results folder\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    fine_tuned_status = \"fine_tuned\" if fine_tuned else \"not_fine_tuned\"\n",
    "    if fine_tuned_status == \"fine_tuned\":\n",
    "        output_dir = f\"{model_name}_{fine_tuned_status}_on_{num_val}\"\n",
    "    else:\n",
    "        output_dir = f\"{model_name}_{fine_tuned_status}\"\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    pynvml.nvmlInit()\n",
    "    inference_times = []\n",
    "    system_metrics = []\n",
    "    \n",
    "    # Process dataset in batches\n",
    "    for i in tqdm(range(0, len(eval_dataset), batch_size), desc=\"Evaluating\", unit=\"batch\"):\n",
    "        batch = eval_dataset[i:i + batch_size]\n",
    "        texts, labels, sources = batch['text'], batch['label'], batch['source']\n",
    "\n",
    "        torch.cuda.synchronize()\n",
    "        start_time = time.time()\n",
    "\n",
    "        for text, label, source in zip(texts, labels, sources):\n",
    "            try:\n",
    "                # Tokenization\n",
    "                inputs = tokenizer(\n",
    "                    [text],\n",
    "                    return_tensors=\"pt\",\n",
    "                    truncation=True,\n",
    "                    padding=True,\n",
    "                    max_length=2048\n",
    "                ).to(model.device)  # Move to model's device\n",
    "\n",
    "                # Generate output\n",
    "                with torch.no_grad():\n",
    "                    outputs = model.generate(\n",
    "                        **inputs,\n",
    "                        max_new_tokens=2,  # Short output\n",
    "                        num_return_sequences=1,\n",
    "                        pad_token_id=tokenizer.pad_token_id,\n",
    "                        eos_token_id=tokenizer.eos_token_id,\n",
    "                        temperature=0.7, \n",
    "                        do_sample=True #generazione deterministica - solo 0 o 1 \n",
    "                    )\n",
    "\n",
    "                # Decode output\n",
    "                generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "                #print(f\"\\nüõ†Ô∏è DEBUG - RAW OUTPUT: '{generated_text}'\")\n",
    "\n",
    "                # Extract response (expecting \"### Response: 0\" or \"### Response: 1\")\n",
    "                generated_ids = outputs[0][inputs.input_ids.shape[1]:]  # Prendi solo i nuovi token\n",
    "                generated_text = tokenizer.decode(generated_ids, skip_special_tokens=True).strip()\n",
    "                #print(f\"Generated text: {generated_text}\")\n",
    "\n",
    "                response = generated_text.strip()\n",
    "                #print(f\"Extracted response: '{response}' | True Label: {label}\")\n",
    "\n",
    "                if response in ['0', '1']:\n",
    "                    predictions.append(int(response))\n",
    "                    true_labels.append(int(label))\n",
    "                    generated_texts.append(generated_text)\n",
    "                    prediction_sources.append(source)\n",
    "                else:\n",
    "                    print(f\"‚ö†Ô∏è Invalid response format: '{response}'\")\n",
    "                    invalid += 1\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Error processing batch: {e}\")\n",
    "                continue\n",
    "        torch.cuda.synchronize()\n",
    "        end_time = time.time()\n",
    "        inference_times.append(end_time - start_time)\n",
    "        cpu_usage = psutil.cpu_percent()\n",
    "        ram_usage = psutil.virtual_memory().percent\n",
    "        gpu_usage = pynvml.nvmlDeviceGetUtilizationRates(pynvml.nvmlDeviceGetHandleByIndex(0)).gpu\n",
    "        system_metrics.append({\"batch\": i//batch_size, \"cpu\": cpu_usage, \"ram\": ram_usage, \"gpu\": gpu_usage, \"time\": end_time - start_time})\n",
    "    # No valid predictions? Return empty results\n",
    "    if not predictions:\n",
    "        print(\"No valid predictions were generated!\")\n",
    "        return None, [], [], []\n",
    "\n",
    "    # Compute metrics\n",
    "    metrics = calculate_metrics(true_labels, predictions)\n",
    "\n",
    "    # Save metrics to CSV\n",
    "    metrics_path_csv = os.path.join(output_dir, \"metrics.csv\")\n",
    "    pd.DataFrame([metrics]).to_csv(metrics_path_csv, index=False)\n",
    "    # Calcola media delle metriche di sistema\n",
    "    if system_metrics:\n",
    "        avg_metrics = {\n",
    "            \"cpu\": sum(m[\"cpu\"] for m in system_metrics) / len(system_metrics),\n",
    "            \"ram\": sum(m[\"ram\"] for m in system_metrics) / len(system_metrics),\n",
    "            \"gpu\": sum(m[\"gpu\"] for m in system_metrics) / len(system_metrics),\n",
    "            \"time\": sum(m[\"time\"] for m in system_metrics) / len(system_metrics),\n",
    "        }\n",
    "\n",
    "        # Salva la media delle metriche in un CSV\n",
    "        avg_metrics_df = pd.DataFrame([avg_metrics])\n",
    "        avg_metrics_path = os.path.join(output_dir, \"avg_system_metrics.csv\")\n",
    "        avg_metrics_df.to_csv(avg_metrics_path, index=False)\n",
    "        print(f\"‚úÖ Media delle metriche salvata in: {avg_metrics_path}\")\n",
    "        print(f\"Avg Inference Time per Batch: {avg_metrics['time']:.4f} sec\")\n",
    "    # Generate confusion matrix\n",
    "    cm = confusion_matrix(true_labels, predictions)\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['0', '1'], yticklabels=['0', '1'])\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    cm_path = os.path.join(output_dir, \"confusion_matrix.png\")\n",
    "    plt.savefig(cm_path, format=\"png\")\n",
    "    plt.close()\n",
    "    print(f\"‚úÖ Confusion matrix saved at: {cm_path}\")\n",
    "\n",
    "    # Display results\n",
    "    print(\"\\nEvaluation Results:\")\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"Samples evaluated: {len(true_labels)}\")\n",
    "    print(f\"Invalid predictions: {invalid}\")\n",
    "    print(\"\\nMetrics:\")\n",
    "    for metric_name, value in metrics.items():\n",
    "        print(f\"{metric_name}: {value:.4f}\")\n",
    "\n",
    "    return metrics, predictions, true_labels, generated_texts, prediction_sources\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def calculate_metrics(true_labels, predictions):\n",
    "    \"\"\"\n",
    "    Calcola le metriche di valutazione, tra cui precisione, recall, e F1-score\n",
    "    con il parametro zero_division per gestire i casi di divisione per zero.\n",
    "    \"\"\"\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(true_labels, predictions),\n",
    "        'precision': precision_score(true_labels, predictions, average='binary', zero_division=0),\n",
    "        'recall': recall_score(true_labels, predictions, average='binary', zero_division=0),\n",
    "        'f1': f1_score(true_labels, predictions, average='binary', zero_division=0)\n",
    "    }\n",
    "\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "secegliere se fare l'evaluation del modello fine tunato o sul modello base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:05<00:00,  1.28s/it]\n"
     ]
    }
   ],
   "source": [
    "max_seq_length = 2048\n",
    "dtype = torch.float16 #altrimenti None\n",
    "load_in_4bit = True\n",
    "seed = 3407\n",
    "load_dotenv()\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "model_name=\"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token = hf_token)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "fine_tuned = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from peft import PeftModel\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "num_val = \"9000\"\n",
    "# Percorso alla cartella dove hai salvato il modello fine-tunato\n",
    "new_model = f\"./fine_tuned_model_llama_3.1_8b_{num_val}\" #i pesi lora\n",
    "tokenizer = AutoTokenizer.from_pretrained(new_model, token = hf_token)\n",
    "\n",
    "max_seq_length = 2048\n",
    "dtype = torch.float16 \n",
    "load_in_4bit = True\n",
    "seed = 3407\n",
    "\n",
    "model_name=\"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "base_model_with_lora_and_ft = PeftModel.from_pretrained(base_model, new_model)\n",
    "#base_model_with_lora_and_ft.eval()\n",
    "\n",
    "fine_tuned = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting evaluation phase...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 282/282 [05:37<00:00,  1.20s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Media delle metriche salvata in: meta-llama/Llama-3.1-8B-Instruct_not_fine_tuned/avg_system_metrics.csv\n",
      "Avg Inference Time per Batch: 1.1942 sec\n",
      "‚úÖ Confusion matrix saved at: meta-llama/Llama-3.1-8B-Instruct_not_fine_tuned/confusion_matrix.png\n",
      "\n",
      "Evaluation Results:\n",
      "Model: meta-llama/Llama-3.1-8B-Instruct\n",
      "Samples evaluated: 2250\n",
      "Invalid predictions: 0\n",
      "\n",
      "Metrics:\n",
      "accuracy: 0.4996\n",
      "precision: 0.4994\n",
      "recall: 0.3733\n",
      "f1: 0.4273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if fine_tuned:\n",
    "    metrics, predictions, true_labels, generated_texts, prediction_sources = evaluate_model(\n",
    "        model=base_model_with_lora_and_ft, tokenizer=tokenizer, eval_dataset=dataset[\"test\"], model_name=model_name, fine_tuned=fine_tuned,num_val=num_val\n",
    "    )\n",
    "else:\n",
    "    metrics, predictions, true_labels, generated_texts, prediction_sources = evaluate_model(\n",
    "        model=model, tokenizer=tokenizer, eval_dataset=dataset[\"test\"], model_name=model_name, fine_tuned=fine_tuned, num_val=0\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
