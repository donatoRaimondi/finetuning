{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torch e CUDA\n",
    "import torch\n",
    "import gc\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "# Transformers e Training\n",
    "from transformers import (\n",
    "    TextStreamer,\n",
    "    TrainingArguments,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    pipeline, # aggiunta ora\n",
    ")\n",
    "from trl import SFTTrainer\n",
    "from peft import LoraConfig, get_peft_model,PeftModel\n",
    "# Dataset e valutazione\n",
    "from datasets import load_dataset, Dataset\n",
    "from evaluate import load\n",
    "import bitsandbytes as bnb\n",
    "# Metriche di valutazione\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_recall_fscore_support,\n",
    "    confusion_matrix\n",
    ")\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Sistema e utility\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Visualizzazione\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name=\"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "if \"meta\" in model_name:\n",
    "    prompt_template = (\n",
    "        \"### Instruction:\\n\"\n",
    "        \"You are an expert software developer and bug triaging specialist. Your task is to predict whether a bug \"\n",
    "        \"will be resolved in LESS than 50 DAYS or MORE than 50 DAYS based on the provided bug details.\\n\\n\"\n",
    "        \n",
    "        \"- Output '0' if the bug will be resolved in LESS than 50 DAYS.\\n\"\n",
    "        \"- Output '1' if the bug will be resolved in MORE than 50 DAYS.\\n\\n\"\n",
    "        \n",
    "        \"Your response MUST be strictly either '0' or '1'. Do NOT include any additional text, explanations, formatting, symbols, or extra characters in your response.\\n\\n\"\n",
    "\n",
    "        \"### Input:\\n\"\n",
    "        \"Source: {source}\\n\"\n",
    "        \"Product: {product}\"\n",
    "        \"Short Description: {short_desc}\\n\"\n",
    "        \"Priority: {priority}\\n\"\n",
    "        \"Severity: {bug_severity}\\n\"\n",
    "        #\"Estimated resolution time: {days_resolution}\\n\\n\" - questo potrebbe influenzare troppo il modello per la predizione\n",
    "\n",
    "        \"### Example Responses:\\n\"\n",
    "        \"Input: Source: KDE | Product: Payment System | Short Description: Critical security vulnerability found in authentication system | Priority: P1 | Severity: Critical\\n\"\n",
    "        \"Output: 0\\n\\n\"\n",
    "        \"Input: Source: OpenOffice | Product: UI Module | Short Description: UI glitch affecting low-impact visual elements in settings panel | Priority: P3 | Severity: Minor\\n\"\n",
    "        \"Output: 1\\n\\n\"\n",
    "\n",
    "        \"### Output: {label}\\n\"\n",
    "    )\n",
    "else:\n",
    "    prompt_template = (\n",
    "        \"You are an expert software developer and bug triaging specialist. Your task is to predict whether a bug \"\n",
    "        \"will be resolved in LESS than 50 DAYS or MORE than 50 DAYS based on the provided bug details.\\n\\n\"\n",
    "        \n",
    "        \"- Output '0' if the bug will be resolved in LESS than 50 DAYS.\\n\"\n",
    "        \"- Output '1' if the bug will be resolved in MORE than 50 DAYS.\\n\\n\"\n",
    "        \n",
    "        \"Your response MUST be strictly either '0' or '1'. Do NOT include any additional text, explanations, formatting, symbols, or extra characters in your response.\\n\\n\"\n",
    "\n",
    "        \"## Input:\\n\"\n",
    "        \"Source: {source}\\n\"\n",
    "        \"Product: {product}\"\n",
    "        \"Short Description: {short_desc}\\n\"\n",
    "        \"Priority: {priority}\\n\"\n",
    "        \"Severity: {bug_severity}\\n\"\n",
    "        #\"Estimated resolution time: {days_resolution}\\n\\n\" - questo potrebbe influenzare troppo il modello per la predizione\n",
    "\n",
    "        \"## Example Responses:\\n\"\n",
    "        \"Input: Source: KDE | Product: Payment System | Short Description: Critical security vulnerability found in authentication system | Priority: P1 | Severity: Critical\\n\"\n",
    "        \"0\\n\\n\"\n",
    "        \"Input: Source: OpenOffice | Product: UI Module | Short Description: UI glitch affecting low-impact visual elements in settings panel | Priority: P3 | Severity: Minor\\n\"\n",
    "        \"1\\n\\n\"\n",
    "\n",
    "        \"## Output:\"\n",
    "        \"{label}\"\n",
    "    )\n",
    "num_val = \"9000\" #1000, 2000, 5000, 9000\n",
    "directory = f\"{model_name}\".split(\"/\")[-1].strip().lower()\n",
    "fine_tuned = True # Imposta a True per valutare il modello fine-tunato, False per il modello base\n",
    "fine_tuned_path = f\"./fine_tuned_model_{directory}_{num_val}\" if fine_tuned else None\n",
    "print(fine_tuned_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_prompts(examples):\n",
    "    texts = []\n",
    "    for source, product, short_desc,priority,bug_severity in zip(\n",
    "        examples[\"source\"],examples[\"product\"], examples[\"short_desc\"],examples[\"priority\"],examples[\"bug_severity\"]\n",
    "    ):\n",
    "        # Costruiamo il prompt\n",
    "        text = prompt_template.format(\n",
    "            source=source,\n",
    "            product=product,\n",
    "            short_desc=short_desc,\n",
    "            priority=priority,\n",
    "            bug_severity=bug_severity,\n",
    "            label=\"\",\n",
    "        )\n",
    "\n",
    "        texts.append(text)\n",
    "    \n",
    "    return {\"text\": texts}\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"csv\",\n",
    "    data_files={\n",
    "        \"test\": f\"../dataset_completo/balanced_datasets/balanced_test.csv\", \n",
    "    },\n",
    ")\n",
    "\n",
    "dataset = dataset.map(formatting_prompts, batched=True)\n",
    "dataset['test'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def calculate_metrics(true_labels, predictions):\n",
    "    \"\"\"\n",
    "    Calcola le metriche di valutazione, tra cui precisione, recall, e F1-score\n",
    "    con il parametro zero_division per gestire i casi di divisione per zero.\n",
    "    \"\"\"\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(true_labels, predictions),\n",
    "        'precision': precision_score(true_labels, predictions, average='binary', zero_division=0),\n",
    "        'recall': recall_score(true_labels, predictions, average='binary', zero_division=0),\n",
    "        'f1': f1_score(true_labels, predictions, average='binary', zero_division=0)\n",
    "    }\n",
    "\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import psutil\n",
    "import pynvml\n",
    "import re\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate_model(model, tokenizer, eval_dataset, model_name, fine_tuned, num_val):\n",
    "    print(\"\\nStarting evaluation phase...\")\n",
    "    \n",
    "    # Move model to evaluation mode\n",
    "    model.eval()  \n",
    "\n",
    "    # Ensure tokenizer padding\n",
    "    tokenizer.padding_side = 'right'\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    predictions, true_labels, generated_texts, prediction_sources = [], [], [], []\n",
    "    batch_size = 8\n",
    "    invalid = 0\n",
    "\n",
    "    # Create results folder\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    fine_tuned_status = \"fine_tuned\" if fine_tuned else \"not_fine_tuned\"\n",
    "    if fine_tuned_status == \"fine_tuned\":\n",
    "        output_dir = f\"{model_name}_{fine_tuned_status}_on_{num_val}\"\n",
    "    else:\n",
    "        output_dir = f\"{model_name}_{fine_tuned_status}\"\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    pynvml.nvmlInit()\n",
    "    inference_times = []\n",
    "    system_metrics = []\n",
    "    \n",
    "    # Process dataset in batches\n",
    "    for i in tqdm(range(0, len(eval_dataset), batch_size), desc=\"Evaluating\", unit=\"batch\"):\n",
    "        batch = eval_dataset[i:i + batch_size]\n",
    "        texts, labels, sources = batch['text'], batch['label'], batch['source']\n",
    "\n",
    "        torch.cuda.synchronize()\n",
    "        start_time = time.time()\n",
    "\n",
    "        for text, label, source in zip(texts, labels, sources):\n",
    "            try:\n",
    "                # Tokenization\n",
    "                inputs = tokenizer(\n",
    "                    [text],\n",
    "                    return_tensors=\"pt\",\n",
    "                    truncation=True,\n",
    "                    padding=True,\n",
    "                    max_length=2048\n",
    "                ).to(model.device)  # Move to model's device\n",
    "\n",
    "                # Generate output\n",
    "                with torch.no_grad():\n",
    "                    outputs = model.generate(\n",
    "                        **inputs,\n",
    "                       max_new_tokens=2,  # Short output\n",
    "                        num_return_sequences=1,\n",
    "                        pad_token_id=tokenizer.pad_token_id,\n",
    "                        eos_token_id=tokenizer.eos_token_id,\n",
    "                        temperature=0.7, \n",
    "                        do_sample=True #generazione deterministica - solo 0 o 1 \n",
    "                    )\n",
    "            \n",
    "                # Decode output\n",
    "                generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "                #print(f\"\\n🛠️ DEBUG - RAW OUTPUT: '{generated_text}'\")\n",
    "\n",
    "                # Extract response (expecting \"### Response: 0\" or \"### Response: 1\")\n",
    "                generated_ids = outputs[0][inputs.input_ids.shape[1]:]  # Prendi solo i nuovi token\n",
    "                generated_text = tokenizer.decode(generated_ids, skip_special_tokens=True).strip()\n",
    "                #print(f\"Generated text: {generated_text}\")\n",
    "\n",
    "                response = generated_text.strip()\n",
    "                #print(f\"Extracted response: '{response}' | True Label: {label}\")\n",
    "\n",
    "                if response in ['0', '1']:\n",
    "                    predictions.append(int(response))\n",
    "                    true_labels.append(int(label))\n",
    "                    generated_texts.append(generated_text)\n",
    "                    prediction_sources.append(source)\n",
    "                else:\n",
    "                    print(f\"⚠️ Invalid response format: '{response}'\")\n",
    "                    invalid += 1\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Error processing batch: {e}\")\n",
    "                continue\n",
    "        torch.cuda.synchronize()\n",
    "        end_time = time.time()\n",
    "        inference_times.append(end_time - start_time)\n",
    "        cpu_usage = psutil.cpu_percent()\n",
    "        ram_usage = psutil.virtual_memory().percent\n",
    "        gpu_usage = pynvml.nvmlDeviceGetUtilizationRates(pynvml.nvmlDeviceGetHandleByIndex(0)).gpu\n",
    "        system_metrics.append({\"batch\": i//batch_size, \"cpu\": cpu_usage, \"ram\": ram_usage, \"gpu\": gpu_usage, \"time\": end_time - start_time})\n",
    "    # No valid predictions? Return empty results\n",
    "    if not predictions:\n",
    "        print(\"No valid predictions were generated!\")\n",
    "        return None, [], [], []\n",
    "\n",
    "    # Compute metrics\n",
    "    metrics = calculate_metrics(true_labels, predictions)\n",
    "\n",
    "    # Save metrics to CSV\n",
    "    metrics_path_csv = os.path.join(output_dir, \"metrics.csv\")\n",
    "    pd.DataFrame([metrics]).to_csv(metrics_path_csv, index=False)\n",
    "    # Calcola media delle metriche di sistema\n",
    "    if system_metrics:\n",
    "        avg_metrics = {\n",
    "            \"cpu\": sum(m[\"cpu\"] for m in system_metrics) / len(system_metrics),\n",
    "            \"ram\": sum(m[\"ram\"] for m in system_metrics) / len(system_metrics),\n",
    "            \"gpu\": sum(m[\"gpu\"] for m in system_metrics) / len(system_metrics),\n",
    "            \"time\": sum(m[\"time\"] for m in system_metrics) / len(system_metrics),\n",
    "        }\n",
    "\n",
    "        # Salva la media delle metriche in un CSV\n",
    "        avg_metrics_df = pd.DataFrame([avg_metrics])\n",
    "        avg_metrics_path = os.path.join(output_dir, \"avg_system_metrics.csv\")\n",
    "        avg_metrics_df.to_csv(avg_metrics_path, index=False)\n",
    "        print(f\"✅ Media delle metriche salvata in: {avg_metrics_path}\")\n",
    "        print(f\"Avg Inference Time per Batch: {avg_metrics['time']:.4f} sec\")\n",
    "    # Generate confusion matrix\n",
    "    cm = confusion_matrix(true_labels, predictions)\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['0', '1'], yticklabels=['0', '1'])\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    cm_path = os.path.join(output_dir, \"confusion_matrix.png\")\n",
    "    plt.savefig(cm_path, format=\"png\")\n",
    "    plt.close()\n",
    "    print(f\"✅ Confusion matrix saved at: {cm_path}\")\n",
    "\n",
    "    # Display results\n",
    "    print(\"\\nEvaluation Results:\")\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"Samples evaluated: {len(true_labels)}\")\n",
    "    print(f\"Invalid predictions: {invalid}\")\n",
    "    print(\"\\nMetrics:\")\n",
    "    for metric_name, value in metrics.items():\n",
    "        print(f\"{metric_name}: {value:.4f}\")\n",
    "\n",
    "    return metrics, predictions, true_labels, generated_texts, prediction_sources\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "secegliere se fare l'evaluation del modello fine tunato o sul modello base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_name, fine_tuned=False, fine_tuned_path=None):\n",
    "    \"\"\"\n",
    "    Carica un modello pre-addestrato o fine-tunato con quantizzazione 4-bit.\n",
    "    \n",
    "    :param model_name: Nome del modello pre-addestrato\n",
    "    :param fine_tuned: Booleano, se True carica il modello fine-tunato\n",
    "    :param fine_tuned_path: Percorso del modello fine-tunato\n",
    "    :param device: Dispositivo su cui caricare il modello ('cuda' o 'cpu')\n",
    "    :return: Modello e tokenizer\n",
    "    \"\"\"\n",
    "    load_dotenv()\n",
    "    hf_token = os.getenv(\"HF_TOKEN\")\n",
    "    \n",
    "    # Configurazione della quantizzazione 4-bit\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "    )\n",
    "    \n",
    "    print(f\"Loading base model: {model_name}\")\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "    \n",
    "    if fine_tuned and fine_tuned_path and os.path.exists(fine_tuned_path):\n",
    "        print(f\"Loading LoRA model from: {fine_tuned_path}\")\n",
    "        model = PeftModel.from_pretrained(base_model, fine_tuned_path)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(fine_tuned_path, token=hf_token)\n",
    "    else:\n",
    "        model = base_model\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name, token=hf_token)\n",
    "\n",
    "    \n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = load_model(model_name, fine_tuned=True, fine_tuned_path=fine_tuned_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if fine_tuned:\n",
    "    metrics, predictions, true_labels, generated_texts, prediction_sources = evaluate_model(\n",
    "        model=model, tokenizer=tokenizer, eval_dataset=dataset[\"test\"], model_name=model_name, fine_tuned=fine_tuned,num_val=num_val\n",
    "    )\n",
    "else:\n",
    "    metrics, predictions, true_labels, generated_texts, prediction_sources = evaluate_model(\n",
    "        model=model, tokenizer=tokenizer, eval_dataset=dataset[\"test\"], model_name=model_name, fine_tuned=fine_tuned, num_val=0\n",
    "    )\n",
    "    # Definiamo le metriche da salvare\n",
    "    training_results = {\n",
    "        \"Dataset Size\": 0,  # Numero di dati usati per il fine-tuning\n",
    "        \"Training Loss\": 0,  # Training Loss\n",
    "        \"Train Time (s)\": 0,  # Tempo di addestramento\n",
    "        \"Steps\": 0,  # Numero di passi (steps)\n",
    "        \"Samples/sec\": 0,  # Campioni al secondo\n",
    "        \"Steps/sec\": 0,  # Passi al secondo\n",
    "        \"Validation Loss\": 0,  # Valutazione della loss\n",
    "    }\n",
    "\n",
    "    # Definiamo il file di destinazione per i risultati\n",
    "    results_file = f\"{model_name}_not_fine_tuned/training_comparison.csv\"\n",
    "\n",
    "    # Converti il dizionario in un DataFrame\n",
    "    training_results_df = pd.DataFrame([training_results])  # Passiamo una lista contenente il dizionario\n",
    "\n",
    "    # Assicura che la cartella esista prima di salvare\n",
    "    Path(results_file).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Salva il DataFrame nel file CSV, sovrascrivendo se già esistente\n",
    "    training_results_df.to_csv(results_file, index=False)\n",
    "\n",
    "    # Mostra la tabella aggiornata\n",
    "    print(training_results_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
