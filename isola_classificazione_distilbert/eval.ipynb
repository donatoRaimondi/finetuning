{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "import time\n",
    "import psutil\n",
    "import pynvml\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Controllare bene questi dati prima di lanciare l'eval sui dati di test sul modello fine tunato o sul modello base:\n",
    "num_val = 1000, 2000, 5000 o 9000\n",
    "fine_tuned = False (se si desidera fare l'eval sul modello \"base\")\n",
    "model_name Ã¨ il nome del modello su cui Ã¨ avvenuto il fine_tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_val = \"9000\" #1000, 2000, 5000, 9000\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "#model_name=\"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "fine_tuned = True # Imposta a True per valutare il modello fine-tunato, False per il modello base\n",
    "fine_tuned_path = f\"./fine_tuned_model_{model_name}_{num_val}\" if fine_tuned else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "# Funzione per caricare il modello\n",
    "def load_model(model_name, fine_tuned=False, fine_tuned_path=None, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Carica un modello pre-addestrato o fine-tunato per la classificazione.\n",
    "    \n",
    "    :param model_name: Nome del modello pre-addestrato (es. 'distilbert-base-uncased')\n",
    "    :param fine_tuned: Booleano, se True carica il modello fine-tunato\n",
    "    :param fine_tuned_path: Percorso del modello fine-tunato\n",
    "    :param device: Dispositivo su cui caricare il modello ('cuda' o 'cpu')\n",
    "    :return: Modello e tokenizer\n",
    "    \"\"\"\n",
    "    id2label = {0: \"fast\", 1: \"slow\"}\n",
    "    label2id = {\"fast\": 0, \"slow\": 1}\n",
    "    load_dotenv()\n",
    "    hf_token = os.getenv(\"HF_TOKEN\")\n",
    "    if fine_tuned and fine_tuned_path and os.path.exists(fine_tuned_path):\n",
    "        print(f\"Loading fine-tuned model from: {fine_tuned_path}\")\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            fine_tuned_path, num_labels=2, id2label=id2label, label2id=label2id\n",
    "        )\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    else:\n",
    "        print(f\"Loading base model: {model_name}\")\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_name, num_labels=2, id2label=id2label, label2id=label2id, token=hf_token\n",
    "        )\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "    device = torch.device(device if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    print(f\"ðŸ“Œ Model loaded on: {device}\")\n",
    "\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funzione per calcolare le metriche\n",
    "def calculate_metrics(true_labels, predictions):\n",
    "    \"\"\"\n",
    "    Calcola accuracy, precision, recall e F1-score tramite la libreria sklearn.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        'accuracy': accuracy_score(true_labels, predictions),\n",
    "        'precision': precision_score(true_labels, predictions, average='binary', zero_division=0),\n",
    "        'recall': recall_score(true_labels, predictions, average='binary', zero_division=0),\n",
    "        'f1': f1_score(true_labels, predictions, average='binary', zero_division=0)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funzione per valutare il modello\n",
    "def evaluate_bert_model(model, tokenizer, eval_dataset, model_name, fine_tuned, num_val):\n",
    "    \"\"\"\n",
    "    Valuta il modello BERT-like su un dataset di test.\n",
    "    \"\"\"\n",
    "    print(\"\\nStarting evaluation phase...\")\n",
    "    model.eval()\n",
    "\n",
    "    # Configura il tokenizer\n",
    "    tokenizer.padding_side = 'right'\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    predictions, true_labels = [], []\n",
    "    batch_size = 8\n",
    "    output_dir = f\"{model_name}_{'fine_tuned' if fine_tuned else 'not_fine_tuned'}_on_{num_val}\" if fine_tuned else f\"{model_name}_not_fine_tuned\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    pynvml.nvmlInit()\n",
    "    inference_times = []\n",
    "    system_metrics = []\n",
    "\n",
    "    # Processa il dataset in batch\n",
    "    for i in tqdm(range(0, len(eval_dataset), batch_size), desc=\"Evaluating\", unit=\"batch\"):\n",
    "        batch = eval_dataset[i:i + batch_size]\n",
    "        texts, labels = batch['text'], batch['label']\n",
    "\n",
    "        torch.cuda.synchronize()\n",
    "        start_time = time.time()\n",
    "\n",
    "        inputs = tokenizer(\n",
    "            texts,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=512\n",
    "        ).to(model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            predictions_batch = torch.argmax(logits, dim=-1).cpu().tolist()\n",
    "\n",
    "        predictions.extend(predictions_batch)\n",
    "        true_labels.extend(labels)\n",
    "\n",
    "        torch.cuda.synchronize()\n",
    "        end_time = time.time()\n",
    "        inference_times.append(end_time - start_time)\n",
    "\n",
    "        cpu_usage = psutil.cpu_percent()\n",
    "        ram_usage = psutil.virtual_memory().percent\n",
    "        gpu_usage = pynvml.nvmlDeviceGetUtilizationRates(pynvml.nvmlDeviceGetHandleByIndex(0)).gpu\n",
    "        system_metrics.append({\"batch\": i // batch_size, \"cpu\": cpu_usage, \"ram\": ram_usage, \"gpu\": gpu_usage, \"time\": end_time - start_time})\n",
    "\n",
    "    # Calcola le metriche\n",
    "    metrics = calculate_metrics(true_labels, predictions)\n",
    "    pd.DataFrame([metrics]).to_csv(os.path.join(output_dir, \"metrics.csv\"), index=False)\n",
    "\n",
    "    # Salva le metriche di sistema\n",
    "    if system_metrics:\n",
    "        avg_metrics = {\n",
    "            \"cpu\": sum(m[\"cpu\"] for m in system_metrics) / len(system_metrics),\n",
    "            \"ram\": sum(m[\"ram\"] for m in system_metrics) / len(system_metrics),\n",
    "            \"gpu\": sum(m[\"gpu\"] for m in system_metrics) / len(system_metrics),\n",
    "            \"time\": sum(m[\"time\"] for m in system_metrics) / len(system_metrics),\n",
    "        }\n",
    "        pd.DataFrame([avg_metrics]).to_csv(os.path.join(output_dir, \"avg_system_metrics.csv\"), index=False)\n",
    "\n",
    "    # Genera la matrice di confusione\n",
    "    cm = confusion_matrix(true_labels, predictions)\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['0', '1'], yticklabels=['0', '1'])\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.savefig(os.path.join(output_dir, \"confusion_matrix.png\"), format=\"png\")\n",
    "    plt.close()\n",
    "\n",
    "    print(\"\\nEvaluation Results:\")\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"Samples evaluated: {len(true_labels)}\")\n",
    "    print(\"\\nMetrics:\")\n",
    "    for metric_name, value in metrics.items():\n",
    "        print(f\"{metric_name}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carica il dataset\n",
    "dataset = load_dataset(\n",
    "    \"csv\",\n",
    "    data_files={\"test\": \"../dataset_completo/balanced_datasets/balanced_test.csv\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepara il dataset\n",
    "def concatenate_fields(example):\n",
    "    fields_to_concat = [\n",
    "        example['source'],\n",
    "        example['product'],\n",
    "        example['short_desc'],\n",
    "        example['priority'],\n",
    "        example['bug_severity'],\n",
    "    ]\n",
    "    example['text'] = ' '.join([str(field) for field in fields_to_concat if field])\n",
    "    return example\n",
    "\n",
    "dataset = dataset.map(concatenate_fields)\n",
    "dataset = dataset.remove_columns(['product', 'short_desc', 'priority', 'bug_severity', 'days_resolution', 'comments'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base model: meta-llama/Llama-3.1-8B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:09<00:00,  2.49s/it]\n",
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.1-8B-Instruct and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Carica il modello\n",
    "model, tokenizer = load_model(model_name, fine_tuned, fine_tuned_path, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizza il dataset\n",
    "def tokenize_function(examples):\n",
    "    #tokenizer.pad_token = tokenizer.eos_token\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_dataset = tokenized_dataset.remove_columns([\"text\"])\n",
    "tokenized_dataset.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting evaluation phase...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2250/2250 [1:27:01<00:00,  2.32s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Results:\n",
      "Model: meta-llama/Llama-3.1-8B-Instruct\n",
      "Samples evaluated: 2250\n",
      "\n",
      "Metrics:\n",
      "accuracy: 0.5062\n",
      "precision: 0.5138\n",
      "recall: 0.2320\n",
      "f1: 0.3197\n"
     ]
    }
   ],
   "source": [
    "# Esegui la valutazione\n",
    "evaluate_bert_model(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    model_name=model_name,\n",
    "    fine_tuned=fine_tuned,\n",
    "    num_val=num_val\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
